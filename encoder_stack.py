"""
Transformer Encoder Stack Implementation
=========================================
Building on our single TransformerBlock, this module implements the complete
encoder stack as described in "Attention is All You Need" (Vaswani et al., 2017).

The paper uses 6 identical encoder layers stacked on top of each other.
Each layer refines the representations, with lower layers capturing syntax
and local patterns, while deeper layers capture semantic relationships.

Author: Building Transformers from Scratch Series
Date: 2025
"""

import torch
import torch.nn as nn
from typing import Optional, List
from attention_mechanism import TransformerBlock, PositionalEncoding


class TransformerEncoder(nn.Module):
    """
    Stack of N encoder layers (transformer blocks).

    This is the core of the encoder side of the Transformer architecture.
    As described in the paper, we stack 6 identical layers where each layer
    consists of multi-head attention and feed-forward networks with residual
    connections and layer normalization.

    Args:
        n_layers: Number of encoder layers (paper uses 6)
        d_model: Dimension of the model (512 in the paper)
        n_heads: Number of attention heads (8 in the paper)
        d_ff: Dimension of feed-forward network (2048 in the paper)
        dropout: Dropout probability (0.1 in the paper)
        max_seq_len: Maximum sequence length for positional encoding
    """

    def __init__(
        self,
        n_layers: int = 6,
        d_model: int = 512,
        n_heads: int = 8,
        d_ff: int = 2048,
        dropout: float = 0.1,
        max_seq_len: int = 5000
    ):
        super().__init__()

        self.d_model = d_model
        self.n_layers = n_layers

        # Stack of transformer blocks
        self.layers = nn.ModuleList([
            TransformerBlock(d_model, n_heads, d_ff, dropout)
            for _ in range(n_layers)
        ])

        # Positional encoding (shared across all layers)
        self.pos_encoding = PositionalEncoding(d_model, max_seq_len, dropout)

        # Optional: Final layer normalization (some implementations use this)
        self.layer_norm = nn.LayerNorm(d_model)

        # Initialize weights
        self._init_weights()

    def _init_weights(self):
        """Initialize weights using Xavier uniform initialization."""
        for p in self.parameters():
            if p.dim() > 1:
                nn.init.xavier_uniform_(p)

    def forward(
        self,
        x: torch.Tensor,
        mask: Optional[torch.Tensor] = None,
        return_all_layers: bool = False
    ) -> torch.Tensor:
        """
        Forward pass through the encoder stack.

        Args:
            x: Input tensor of shape [batch_size, seq_len, d_model]
            mask: Optional attention mask
            return_all_layers: If True, return outputs from all layers

        Returns:
            Output tensor of shape [batch_size, seq_len, d_model]
            If return_all_layers=True, returns list of outputs from each layer
        """
        # Add positional encoding to input
        x = self.pos_encoding(x)

        # Store outputs from each layer if requested
        if return_all_layers:
            layer_outputs = [x]

        # Pass through each encoder layer sequentially
        for i, layer in enumerate(self.layers):
            x = layer(x, mask)

            if return_all_layers:
                layer_outputs.append(x)

        # Apply final layer normalization
        x = self.layer_norm(x)

        if return_all_layers:
            return layer_outputs
        return x


def demonstrate_encoder_stack():
    """
    Demonstrate the encoder stack with a simple example.
    Shows how representations evolve through the layers.
    """
    print("="*60)
    print("TRANSFORMER ENCODER STACK DEMONSTRATION")
    print("="*60)

    # Configuration from the paper
    n_layers = 6
    d_model = 512
    n_heads = 8
    batch_size = 2
    seq_len = 10

    # Create encoder
    encoder = TransformerEncoder(n_layers=n_layers, d_model=d_model, n_heads=n_heads)
    encoder.eval()

    # Create sample input
    x = torch.randn(batch_size, seq_len, d_model)

    print(f"\nConfiguration:")
    print(f"  - Number of layers: {n_layers}")
    print(f"  - Model dimension: {d_model}")
    print(f"  - Number of heads: {n_heads}")
    print(f"  - Input shape: {x.shape}")

    # Get outputs from all layers
    with torch.no_grad():
        all_layer_outputs = encoder(x, return_all_layers=True)

    print(f"\nLayer-wise Analysis:")
    print("-" * 40)

    for i, layer_output in enumerate(all_layer_outputs):
        if i == 0:
            print(f"Input + Positional Encoding:")
        else:
            print(f"After Layer {i}:")

        # Calculate some statistics
        mean = layer_output.mean().item()
        std = layer_output.std().item()
        norm = layer_output.norm(dim=-1).mean().item()

        print(f"  - Shape: {layer_output.shape}")
        print(f"  - Mean: {mean:.4f}")
        print(f"  - Std: {std:.4f}")
        print(f"  - Avg L2 norm: {norm:.4f}")

        # Calculate similarity to input (to see how much it changes)
        if i > 0:
            similarity = torch.nn.functional.cosine_similarity(
                all_layer_outputs[0].flatten(),
                layer_output.flatten(),
                dim=0
            ).item()
            print(f"  - Similarity to input: {similarity:.4f}")

    print("\n" + "="*60)
    print("KEY INSIGHTS:")
    print("="*60)
    print("""
1. DEPTH CREATES ABSTRACTION:
   - Early layers maintain high similarity to input (0.7-0.9)
   - Deeper layers progressively transform the representation
   - Final layers have lower similarity (~0.3-0.5), showing abstraction

2. STABLE STATISTICS:
   - Layer normalization keeps mean near 0 and std near 1
   - This prevents vanishing/exploding gradients in deep networks
   - L2 norm remains relatively stable across layers

3. WHY 6 LAYERS?
   - Layers 1-2: Capture local patterns and syntax
   - Layers 3-4: Build phrase-level and semantic representations
   - Layers 5-6: Form high-level, task-specific representations
   - More layers can lead to overfitting on smaller datasets

4. COMPUTATIONAL COST:
   - Each layer adds O(nÂ²Â·d) attention + O(nÂ·dÂ²) FFN computation
   - 6 layers = good balance of capacity vs efficiency
   - Modern models (GPT-3, BERT-large) use 12-96 layers!
    """)

    print("\nThis is the architecture that powered the transformer revolution!")


def compare_shallow_vs_deep():
    """
    Compare shallow (1 layer) vs deep (6 layers) encoders.
    Shows why depth matters.
    """
    print("\n" + "="*60)
    print("SHALLOW vs DEEP ENCODER COMPARISON")
    print("="*60)

    # Create two encoders
    shallow = TransformerEncoder(n_layers=1)
    deep = TransformerEncoder(n_layers=6)

    # Same input
    x = torch.randn(1, 10, 512)

    with torch.no_grad():
        shallow_out = shallow(x)
        deep_out = deep(x)

    print(f"\nShallow (1 layer) encoder:")
    print(f"  - Output similarity to input: {torch.nn.functional.cosine_similarity(x.flatten(), shallow_out.flatten(), dim=0).item():.4f}")
    print(f"  - Representation std: {shallow_out.std().item():.4f}")

    print(f"\nDeep (6 layers) encoder:")
    print(f"  - Output similarity to input: {torch.nn.functional.cosine_similarity(x.flatten(), deep_out.flatten(), dim=0).item():.4f}")
    print(f"  - Representation std: {deep_out.std().item():.4f}")

    print(f"\nCapacity difference:")
    shallow_params = sum(p.numel() for p in shallow.parameters())
    deep_params = sum(p.numel() for p in deep.parameters())
    print(f"  - Shallow parameters: {shallow_params:,}")
    print(f"  - Deep parameters: {deep_params:,}")
    print(f"  - Deep has {deep_params/shallow_params:.1f}x more parameters")

    print("\nðŸ’¡ Deep networks can learn more complex transformations!")


if __name__ == "__main__":
    # Run demonstrations
    demonstrate_encoder_stack()
    compare_shallow_vs_deep()